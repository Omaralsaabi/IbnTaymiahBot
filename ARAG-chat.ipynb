{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "from torch import cuda\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "import os\n",
    "import time\n",
    "import pinecone\n",
    "from datasets import load_dataset\n",
    "from tqdm.auto import tqdm\n",
    "from torch import cuda, bfloat16\n",
    "import transformers\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.chains import RetrievalQA\n",
    "import pandas as pd\n",
    "import re \n",
    "from maha.cleaners.functions import remove, normalize\n",
    "\n",
    "\n",
    "embed_model_id = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 3 embeddings, each with a dimensionality of 384\n"
     ]
    }
   ],
   "source": [
    "embed_model = HuggingFaceEmbeddings(\n",
    "    model_name=embed_model_id,\n",
    "    model_kwargs={'device': device},\n",
    "    encode_kwargs={'device': device, 'batch_size': 32}\n",
    ")\n",
    "\n",
    "\n",
    "docs = [\n",
    "    'هذه هي الجملة الأولى',\n",
    "    'هذه هي الجملة الثانية',\n",
    "    'هذه هي الجملة الثالثة',\n",
    "]\n",
    "\n",
    "embeddings = embed_model.embed_documents(docs)\n",
    "\n",
    "print(f\"We have {len(embeddings)} embeddings, each with \" f\"a dimensionality of {len(embeddings[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {},\n",
      " 'total_vector_count': 0}\n"
     ]
    }
   ],
   "source": [
    "pinecone.init(\n",
    "    api_key='api-key',\n",
    "    environment='us-west4-gcp-free'\n",
    ")\n",
    "\n",
    "index_name = 'arag-chat'\n",
    "\n",
    "if index_name not in pinecone.list_indexes():\n",
    "    pinecone.create_index(\n",
    "        index_name,\n",
    "        dimension=len(embeddings[0]),\n",
    "        metric='cosine',\n",
    "    )\n",
    "\n",
    "    while not pinecone.describe_index(index_name).status['ready']:\n",
    "        time.sleep(1)\n",
    "\n",
    "index = pinecone.Index(index_name)\n",
    "\n",
    "print(index.describe_index_stats())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters in the text: 25926305\n"
     ]
    }
   ],
   "source": [
    "with open('data/مجموع الفتاوى.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(f\"Number of characters in the text: {len(text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of paragraphs: 565\n"
     ]
    }
   ],
   "source": [
    "paragraphs = text.split(\"_________\")\n",
    "\n",
    "print(f\"Number of paragraphs: {len(paragraphs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\"ID\": range(1, len(paragraphs) + 1), \"Paragraph\": paragraphs}\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_re = re.compile(pattern=\"#[\\w\\d]+\")\n",
    "\n",
    "def remove_hashtag(text: str) -> str:\n",
    "    return hashtag_re.sub(repl=\"\", string=text)\n",
    "\n",
    "mention_re = re.compile(\"\\B@\\w+\")\n",
    "def remove_mention(text: str) -> str:\n",
    "    return mention_re.sub(repl=\"\", string=text)\n",
    "\n",
    "punc_re = re.compile(r\"\"\"[!\"#$%&\\'()*+,-./:;<=>?@[\\\\\\]^_`{|}~،؟…«“\\\":\\\"…”]\"\"\")\n",
    "def remove_punctation(text: str) -> str:\n",
    "    return punc_re.sub(repl=\"\", string=text)\n",
    "\n",
    "url_re = re.compile(r\"https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)\")\n",
    "def remove_urls(text: str) -> str:\n",
    "    return url_re.sub(repl=\"\", string=text)\n",
    "\n",
    "diactircs_re = re.compile(\"[\\u064B-\\u0652]\")\n",
    "\n",
    "def remove_diactrics(text: str) -> str:\n",
    "    return diactircs_re.sub(repl=\"\", string=text)\n",
    "\n",
    "numbers_re = re.compile(\"\\d\")\n",
    "def remove_numbers(text: str) -> str:\n",
    "    return numbers_re.sub(repl=\"\", string=text)\n",
    "\n",
    "english_chars_re = re.compile(\"[A-Za-z]\")\n",
    "def remove_english_characters(text: str) -> str:\n",
    "    return english_chars_re.sub(repl=\"\", string=text)\n",
    "\n",
    "multiple_space_re = re.compile(\"\\s{2,}\")\n",
    "def remove_multiple_whitespace(text: str) -> str:\n",
    "    return multiple_space_re.sub(repl=\" \", string=text)\n",
    "\n",
    "\n",
    "def clean_all(text: str) -> str:\n",
    "    text = remove_hashtag(text)\n",
    "    text = remove_mention(text)\n",
    "    text = remove_punctation(text)\n",
    "    text = remove_urls(text)\n",
    "    text = remove_diactrics(text)\n",
    "    text = remove_numbers(text)\n",
    "    text = remove_english_characters(text)\n",
    "    text = remove_multiple_whitespace(text)\n",
    "    text = text.strip()\n",
    "    \n",
    "    text = remove(text=text, all_harakat=True, tatweel=True, punctuations=True)\n",
    "    text = normalize(text=text, all=True)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned'] = df['Paragraph'].apply(clean_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Paragraph</th>\n",
       "      <th>cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>الْجُزْءُ الْأَوَّلُ\\nكِتَابُ تَوْحِيدِ الْأُل...</td>\n",
       "      <td>الجزء الاول\\nكتاب توحيد الالوهيه\\nقال شيخ الاس...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>\\n&lt;s0&gt;\\nبياض بالأصل، والزيادة من الحاكم في الت...</td>\n",
       "      <td>بياض بالاصل والزياده من الحاكم في التفسير وقال...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>\\n&lt;s0&gt;\\n(١)  بياض بالأصل\\nقال الشيخ ناصر بن حم...</td>\n",
       "      <td>بياض بالاصل\\nقال الشيخ ناصر بن حمد الفهد ص وال...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>\\n&lt;s0&gt;\\n(*)  قال الشيخ ناصر بن حمد الفهد (ص ١٤...</td>\n",
       "      <td>قال الشيخ ناصر بن حمد الفهد ص قلت وهنا امران\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>\\n&lt;s0&gt;\\n(١)  هكذا بالأصلإلَيْهِ. وَالْخَلْقُ: ...</td>\n",
       "      <td>هكذا بالاصلاليه والخلق اهون ما يكون عليهم احوج...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                          Paragraph  \\\n",
       "0   1  الْجُزْءُ الْأَوَّلُ\\nكِتَابُ تَوْحِيدِ الْأُل...   \n",
       "1   2  \\n<s0>\\nبياض بالأصل، والزيادة من الحاكم في الت...   \n",
       "2   3  \\n<s0>\\n(١)  بياض بالأصل\\nقال الشيخ ناصر بن حم...   \n",
       "3   4  \\n<s0>\\n(*)  قال الشيخ ناصر بن حمد الفهد (ص ١٤...   \n",
       "4   5  \\n<s0>\\n(١)  هكذا بالأصلإلَيْهِ. وَالْخَلْقُ: ...   \n",
       "\n",
       "                                             cleaned  \n",
       "0  الجزء الاول\\nكتاب توحيد الالوهيه\\nقال شيخ الاس...  \n",
       "1  بياض بالاصل والزياده من الحاكم في التفسير وقال...  \n",
       "2  بياض بالاصل\\nقال الشيخ ناصر بن حمد الفهد ص وال...  \n",
       "3  قال الشيخ ناصر بن حمد الفهد ص قلت وهنا امران\\n...  \n",
       "4  هكذا بالاصلاليه والخلق اهون ما يكون عليهم احوج...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data/مجموع الفتاوى_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 1/9 [00:02<00:18,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed 64 paragraphs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 2/9 [00:04<00:13,  1.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed 128 paragraphs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 3/9 [00:05<00:10,  1.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed 192 paragraphs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 4/9 [00:07<00:09,  1.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed 256 paragraphs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 5/9 [00:09<00:07,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed 320 paragraphs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 6/9 [00:11<00:05,  1.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed 384 paragraphs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 7/9 [00:13<00:03,  1.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed 448 paragraphs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 8/9 [00:15<00:01,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed 512 paragraphs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:16<00:00,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed 565 paragraphs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 565}},\n",
      " 'total_vector_count': 565}\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "for i in tqdm(range(0,len(df), batch_size)):\n",
    "    i_end = min(len(df), i + batch_size)\n",
    "    batch = df.iloc[i:i_end]\n",
    "\n",
    "    ids = [f\"{x['ID']}\" for _, x in batch.iterrows()]\n",
    "    paragraphs = [x['cleaned'] for _, x in batch.iterrows()]\n",
    "    embeds = embed_model.embed_documents(paragraphs)\n",
    "\n",
    "    metadata = [\n",
    "        {\n",
    "            'id': x['ID'],\n",
    "            'paragraph': x['Paragraph'][:200]\n",
    "        }\n",
    "        for _, x in batch.iterrows()\n",
    "    ]\n",
    "    index.upsert(vectors=zip(ids, embeds, metadata))\n",
    "    print(f\"Indexed {i_end} paragraphs\")\n",
    "\n",
    "print(index.describe_index_stats())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
